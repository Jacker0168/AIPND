{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Outline key points for neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perceptron\n",
    "1. Onehot encode (for input)  \n",
    "pd.get_dummies(data)  \n",
    "sklearn.preprocessing.OneHotEncoder(sparse = False).fit_transform(data)\n",
    "2. Normallization (for input)\n",
    "3. linear combination (for output)\n",
    "4. step function (discrete output)\n",
    "5. sigmoid function (continuous output)\n",
    "$$S(t)=\\frac{1}{1+e^{-t}}$$\n",
    "$$S'(t)=S(t)(1-S(t))$$\n",
    "6. softmax function (multi-classification problem)\n",
    "$$Softmax(x_i) = \\frac{e^{x_i}}{\\sum_i e^{x_i}}$$\n",
    "7. percrptron algo (adjustment)\n",
    "8. Maximum Likelihood (log transformation)\n",
    "9. cross entropy ~ $\\frac{1}{correct\\space probability}$  \n",
    "For 0-1 problem: \n",
    "$$Cross-Entropy(y, p) = -\\sum_{i=1}^m y'_iln(y_i) + (1-y'_i)ln(1-y_i)$$\n",
    "where $y_i$ is the predicted probability value for class i and $y'_i$ is the true probability for that class. m is data size.  \n",
    "For multi-class problem:\n",
    "$$Cross-Entropy = \\sum_{i=1}^m \\sum_{j=1}^n -y'^{(j)}_i ln(y^{(j)}_i)$$\n",
    "where n is classes\n",
    "10. Error Function\n",
    "$$E(w, b) = -\\frac{1}{m} \\sum_{i=1}^m y'_i ln(\\sigma(wx_i+b)) + (1-y'_i)ln(1-\\sigma(wx_i+b))$$\n",
    "11. Gradient Descent\n",
    "\n",
    "### Neural Network\n",
    "1. Feedforward Neural Network\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
      "       handle_unknown='error', n_values='auto', sparse=True)\n",
      "[2 3 4]\n",
      "[0 2 5 9]\n",
      "[[1. 0. 0. 1. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Onehot\n",
    "# example 1:\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "data = pd.read_excel('data.xlsx')\n",
    "\n",
    "method_pd = pd.get_dummies(data, columns = ['y'])\n",
    "sklearn_onehot = preprocessing.OneHotEncoder(sparse = False)\n",
    "method_sklearn = sklearn_onehot.fit_transform(data['y'].values.reshape(-1, 1))\n",
    "\n",
    "# example 2:\n",
    "print('Example 2:')\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "print(enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]))\n",
    "print(enc.n_values_)\n",
    "print(enc.feature_indices_)\n",
    "print(enc.transform([[0, 1, 1]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems when training neural network\n",
    "1. overfitting & underfitting  \n",
    "(1) Model complexity graph (determine epoch, early stopping)  \n",
    "(2) L1(feature selection), L2(use most) Regularization (Penalize for large weights)  \n",
    "(3) Dropout: randomly close some nodes in each epoch  \n",
    "prob each node will be dropout = 0.2 20% nodes will be turned off\n",
    "2. bias & variance\n",
    "3. gradient descent(local minimum, gradient disappear)  \n",
    "(1) activate function(tanh, relu)  \n",
    "(2) stochastic gradient descent(mini-batch, different dataset for each epoch)  \n",
    "(3) learning rate(decrease learning rate when model not good)\n",
    "(4) random restart\n",
    "(5) momentum $\\beta$\n",
    "$$STEP(n)=STEP(n)+\\beta STEP(n-1)+\\beta^2STEP(n-2)+...$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. perceptron algorithm\n",
    "* step 1: start with random weights: w1, ..., wn, b  \n",
    "* step 2: for every misclassified point  \n",
    "```\n",
    "if prediction = 0:\n",
    "    for i in range(n)\n",
    "        change wi to wi + axi\n",
    "        change b to b + a\n",
    "where a is learning rate\n",
    "if prediction = 1:\n",
    "    for i in range(n)\n",
    "        change wi to wi - axi\n",
    "        change b to b - a```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent\n",
    "* step 1: start with random weights: w1, ..., wn, b  \n",
    "* step 2: for every point $(x_1, x_2, ..., x_n)$  \n",
    "```\n",
    "For i in range(n):\n",
    "    Update wi to wi - a*partial(E)/partial(wi)\n",
    "    Update b to b - a*partial(E)/partial(b)\n",
    "```\n",
    "* Workshop: Implementing the Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Backpropagation\n",
    "* Workshop: Student Admissions\n",
    "* Reference:  \n",
    "(1) https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/  \n",
    "(2) http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
